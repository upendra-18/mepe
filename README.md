# Multimodal Emotion Persona Engine (MEPE)

MEPE is an end-to-end multimodal AI system that understands a user‚Äôs emotional state and communication persona from text and facial expressions, fuses these signals into a unified representation, and generates emotion-aware, persona-aligned responses using a large language model.

This project demonstrates production-style AI system design combining NLP, Computer Vision, Multimodal Fusion, and Controlled Text Generation.

Key Skills Demonstrated (ATS-Optimized)

Natural Language Processing (NLP)

Computer Vision (CNNs)

Transformer Models (DistilBERT, T5)

Multimodal Representation Learning

Emotion Recognition

Persona Modeling

Gated Multimodal Fusion

Prompt Engineering

Controlled Text Generation

TensorFlow / Keras

Hugging Face Transformers

End-to-End ML System Design

System Architecture Overview
## üß© Architecture Diagram

The following diagram illustrates the end-to-end flow of the Multimodal Emotion Persona Engine (MEPE), from raw inputs to persona-aware response generation.

![MEPE Architecture Diagram](assets/architecture.png)

Project Structure
mepe/
‚îú‚îÄ‚îÄ phase1_text_emotion/        # Text emotion classification (Transformer)
‚îú‚îÄ‚îÄ phase2_face_emotion/        # Facial emotion recognition (CNN)
‚îú‚îÄ‚îÄ phase3_inference/           # Independent inference pipelines
‚îú‚îÄ‚îÄ phase4_fusion/              # Multimodal fusion (gated attention)
‚îú‚îÄ‚îÄ phase5_persona_llm/
‚îÇ   ‚îî‚îÄ‚îÄ phase5_persona_llm.ipynb  # 5A + 5B + 5C (design ‚Üí control ‚Üí demo)
‚îî‚îÄ‚îÄ README.md

Note: Phase 5 (Design, Persona Control, and Demo) is intentionally implemented in a single notebook to preserve end-to-end reasoning and reproducibility.

Phase Breakdown
Phase 1 ‚Äì Text Emotion Modeling

Transformer-based emotion classification using DistilBERT

Fine-tuned on multi-label emotion datasets

Outputs dense text emotion embeddings

Phase 2 ‚Äì Face Emotion Modeling

CNN-based facial emotion recognition (FER-2013)

Outputs facial emotion probabilities and embeddings

Phase 3 ‚Äì Independent Inference

Standalone inference for text and face emotion models

Model persistence and reproducibility

Phase 4 ‚Äì Multimodal Fusion

Gated Fusion Mechanism combines text and face embeddings

Learns modality importance dynamically

Produces a unified persona embedding

Phase 5 ‚Äì Persona-Aware LLM Generation

Implemented in phase5_persona_llm.ipynb:

5A ‚Äì Design: Persona schema, control variables, LLM selection

5B ‚Äì Persona Control: Rule-based policy mapping persona traits ‚Üí behavioral controls

5C ‚Äì Demo: End-to-end emotion-aware response generation

üîç Demo (Research-Grade)
User Input (Text)

‚ÄúI feel overwhelmed and frustrated with how things are going.‚Äù

Detected Persona (Multimodal)

Stress: Medium

Sadness: Medium

Emotional Intensity: Medium

Confidence: High

Formality Preference: Low

Derived Control Signals

Empathy Level: Medium

Response Style: Calm

Assertiveness: Medium

Formality: Casual

Generated Response

‚ÄúIt sounds like you‚Äôre dealing with a lot right now, and that can be genuinely exhausting. It might help to slow things down and focus on one small step at a time. If you‚Äôd like, we can think through what would make things feel more manageable.‚Äù

The response style is dynamically controlled by multimodal emotion signals and a persona-aware policy, not static prompting.

Why Gated Fusion (Design Decision)

Handles noisy or missing modalities

Computationally efficient compared to cross-attention

Interpretable modality weighting

Suitable for single-GPU environments (Colab / Kaggle)

This mirrors real-world system trade-offs in applied AI teams.

Real-World Applications

Emotion-aware conversational agents

Mental health and wellbeing assistants

Adaptive customer support systems

Human-centric AI interfaces

Personalized AI companions

Limitations & Future Work

Replace rule-based persona control with learned policy

Temporal modeling of emotion drift

Reinforcement learning for empathy optimization

Cross-attention fusion for long multimodal sequences

Real-time video-based inference

Reproducibility

All models, inference steps, and demos are runnable via the provided notebooks.
Pretrained models are loaded using Hugging Face‚Äìcompatible formats.

Author Notes (Optional but Strong)

This project was built end-to-end by a single developer, emphasizing system-level thinking, engineering trade-offs, and research-inspired design over isolated model performance.
