{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14158778,"sourceType":"datasetVersion","datasetId":9024490}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall -y protobuf tensorflow-metadata\n!pip install protobuf==3.20.3 tensorflow-metadata==1.13.1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T02:59:03.025449Z","iopub.execute_input":"2025-12-15T02:59:03.026536Z","iopub.status.idle":"2025-12-15T02:59:11.707667Z","shell.execute_reply.started":"2025-12-15T02:59:03.026501Z","shell.execute_reply":"2025-12-15T02:59:11.706224Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: protobuf 6.33.0\nUninstalling protobuf-6.33.0:\n  Successfully uninstalled protobuf-6.33.0\nFound existing installation: tensorflow-metadata 1.17.2\nUninstalling tensorflow-metadata-1.17.2:\n  Successfully uninstalled tensorflow-metadata-1.17.2\nCollecting protobuf==3.20.3\n  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\nCollecting tensorflow-metadata==1.13.1\n  Downloading tensorflow_metadata-1.13.1-py3-none-any.whl.metadata (2.1 kB)\nRequirement already satisfied: absl-py<2.0.0,>=0.9 in /usr/local/lib/python3.11/dist-packages (from tensorflow-metadata==1.13.1) (1.4.0)\nRequirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-metadata==1.13.1) (1.70.0)\nDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading tensorflow_metadata-1.13.1-py3-none-any.whl (28 kB)\nInstalling collected packages: protobuf, tensorflow-metadata\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed protobuf-3.20.3 tensorflow-metadata-1.13.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\n\nprint(os.listdir(\"/kaggle/input/mepe-text-emotion-hf\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T02:59:11.709959Z","iopub.execute_input":"2025-12-15T02:59:11.710895Z","iopub.status.idle":"2025-12-15T02:59:11.733732Z","shell.execute_reply.started":"2025-12-15T02:59:11.710850Z","shell.execute_reply":"2025-12-15T02:59:11.732560Z"}},"outputs":[{"name":"stdout","text":"['__huggingface_repos__.json', 'mepe']\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\n\nBASE = \"/kaggle/input/mepe-text-emotion-hf/mepe/models/text_emotion_hf\"\n\nprint(os.listdir(BASE))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T02:59:11.734963Z","iopub.execute_input":"2025-12-15T02:59:11.735454Z","iopub.status.idle":"2025-12-15T02:59:11.747050Z","shell.execute_reply.started":"2025-12-15T02:59:11.735418Z","shell.execute_reply":"2025-12-15T02:59:11.745675Z"}},"outputs":[{"name":"stdout","text":"['config.json', 'tokenizer.json', 'tf_model.h5', 'tokenizer_config.json', 'special_tokens_map.json', 'vocab.txt']\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"we got the output of the phase1: text_emotion.ipynb, by inputting it to the current notebook as a datset after exporting it through the phase1: text_emotion.ipynb as datset","metadata":{}},{"cell_type":"markdown","source":"# imports","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport os\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-15T02:59:11.749114Z","iopub.execute_input":"2025-12-15T02:59:11.749464Z","iopub.status.idle":"2025-12-15T02:59:34.327328Z","shell.execute_reply.started":"2025-12-15T02:59:11.749432Z","shell.execute_reply":"2025-12-15T02:59:34.326317Z"}},"outputs":[{"name":"stderr","text":"2025-12-15 02:59:13.764788: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765767554.041308      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765767554.117309      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Paths & Config","metadata":{}},{"cell_type":"code","source":"\n\nTEXT_DIM = 768\nFACE_DIM = 256\nFUSION_DIM = 512\nNUM_EMOTIONS = 28   # text emotion space\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T02:59:34.328669Z","iopub.execute_input":"2025-12-15T02:59:34.329311Z","iopub.status.idle":"2025-12-15T02:59:34.334601Z","shell.execute_reply.started":"2025-12-15T02:59:34.329277Z","shell.execute_reply":"2025-12-15T02:59:34.333393Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Load pretrained encoders (FROZEN)\nText encoder (DistilBERT backbone only)","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, TFDistilBertModel\n\nHF_TEXT_DIR = \"/kaggle/input/mepe-text-emotion-hf/mepe/models/text_emotion_hf\"\n\ntokenizer = AutoTokenizer.from_pretrained(\n    HF_TEXT_DIR,\n    local_files_only=True\n)\n\ntext_encoder = TFDistilBertModel.from_pretrained(\n    HF_TEXT_DIR,\n    local_files_only=True\n)\n\nprint(\"HF model accessible\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T02:59:34.335556Z","iopub.execute_input":"2025-12-15T02:59:34.335889Z","iopub.status.idle":"2025-12-15T02:59:53.996350Z","shell.execute_reply.started":"2025-12-15T02:59:34.335864Z","shell.execute_reply":"2025-12-15T02:59:53.995463Z"}},"outputs":[{"name":"stderr","text":"2025-12-15 02:59:50.024334: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\nTensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\nSome layers from the model checkpoint at /kaggle/input/mepe-text-emotion-hf/mepe/models/text_emotion_hf were not used when initializing TFDistilBertModel: ['classifier', 'dropout_19', 'pre_classifier']\n- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFDistilBertModel were initialized from the model checkpoint at /kaggle/input/mepe-text-emotion-hf/mepe/models/text_emotion_hf.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"HF model accessible\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Face embedding model","metadata":{}},{"cell_type":"code","source":"face_encoder = tf.keras.models.load_model(FACE_EMB_DIR)\nface_encoder.trainable = False\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T02:59:53.997594Z","iopub.execute_input":"2025-12-15T02:59:53.997926Z","iopub.status.idle":"2025-12-15T02:59:54.246025Z","shell.execute_reply.started":"2025-12-15T02:59:53.997900Z","shell.execute_reply":"2025-12-15T02:59:54.244297Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/2485425292.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mface_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFACE_EMB_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mface_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'FACE_EMB_DIR' is not defined"],"ename":"NameError","evalue":"name 'FACE_EMB_DIR' is not defined","output_type":"error"}],"execution_count":7},{"cell_type":"markdown","source":"# Gated Fusion Layer (CORE LOGIC)","metadata":{}},{"cell_type":"code","source":"class GatedFusion(tf.keras.layers.Layer):\n    def __init__(self, fusion_dim):\n        super().__init__()\n        self.text_proj = tf.keras.layers.Dense(fusion_dim)\n        self.face_proj = tf.keras.layers.Dense(fusion_dim)\n        self.gate = tf.keras.layers.Dense(fusion_dim, activation=\"sigmoid\")\n\n    def call(self, text_emb, face_emb):\n        t = self.text_proj(text_emb)\n        f = self.face_proj(face_emb)\n        g = self.gate(tf.concat([t, f], axis=-1))\n        return g * t + (1 - g) * f\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T02:59:54.246803Z","iopub.status.idle":"2025-12-15T02:59:54.247291Z","shell.execute_reply.started":"2025-12-15T02:59:54.247053Z","shell.execute_reply":"2025-12-15T02:59:54.247075Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Build Fusion Model","metadata":{}},{"cell_type":"code","source":"# Inputs\ntext_input_ids = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"input_ids\")\ntext_mask = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"attention_mask\")\nface_input = tf.keras.Input(shape=(256,), name=\"face_embedding\")\n\n# Text encoding\ntext_out = text_encoder(\n    input_ids=text_input_ids,\n    attention_mask=text_mask\n).last_hidden_state[:, 0, :]   # CLS token\n\n# Fusion\nfusion_layer = GatedFusion(FUSION_DIM)\nfusion_emb = fusion_layer(text_out, face_input)\n\n# Output head\nx = tf.keras.layers.Dense(256, activation=\"relu\")(fusion_emb)\nlogits = tf.keras.layers.Dense(NUM_EMOTIONS, name=\"emotion_logits\")(x)\n\nfusion_model = tf.keras.Model(\n    inputs=[text_input_ids, text_mask, face_input],\n    outputs=logits\n)\n\nfusion_model.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T02:59:54.249775Z","iopub.status.idle":"2025-12-15T02:59:54.250138Z","shell.execute_reply.started":"2025-12-15T02:59:54.249987Z","shell.execute_reply":"2025-12-15T02:59:54.250003Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# compile","metadata":{}},{"cell_type":"code","source":"fusion_model.compile(\n    optimizer=tf.keras.optimizers.Adam(2e-4),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=[\"accuracy\"]\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T02:59:54.251139Z","iopub.status.idle":"2025-12-15T02:59:54.251512Z","shell.execute_reply.started":"2025-12-15T02:59:54.251362Z","shell.execute_reply":"2025-12-15T02:59:54.251376Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dummy sanity test \n\nBefore real training, verify forward pass.","metadata":{}},{"cell_type":"code","source":"dummy_text = tokenizer(\n    [\"I feel very anxious today\"],\n    return_tensors=\"tf\",\n    padding=True\n)\n\ndummy_face = tf.random.normal((1, 256))\n\nout = fusion_model([\n    dummy_text[\"input_ids\"],\n    dummy_text[\"attention_mask\"],\n    dummy_face\n])\n\nout.shape\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T02:59:54.252685Z","iopub.status.idle":"2025-12-15T02:59:54.252994Z","shell.execute_reply.started":"2025-12-15T02:59:54.252863Z","shell.execute_reply":"2025-12-15T02:59:54.252877Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training strategy (READ THIS)\n\nAt Phase 4:\n\nWe do NOT retrain encoders\n\nWe train only fusion layers\n\nThis avoids:\n\ninstability\n\nmassive compute\n\noverfitting\n\nWe will train on paired (text, face) samples later\n(real or synthetic).\n\nFor now, architecture correctness is the goal.","metadata":{}},{"cell_type":"markdown","source":"# Save fusion model","metadata":{}},{"cell_type":"code","source":"os.makedirs(FUSION_DIR, exist_ok=True)\nfusion_model.save(FUSION_DIR)\n\nprint(\"Fusion model saved.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T02:59:54.255423Z","iopub.status.idle":"2025-12-15T02:59:54.255926Z","shell.execute_reply.started":"2025-12-15T02:59:54.255669Z","shell.execute_reply":"2025-12-15T02:59:54.255691Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Persona embedding extractor","metadata":{}},{"cell_type":"code","source":"persona_model = tf.keras.Model(\n    inputs=fusion_model.inputs,\n    outputs=fusion_emb\n)\n\npersona_model.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T02:59:54.256978Z","iopub.status.idle":"2025-12-15T02:59:54.257730Z","shell.execute_reply.started":"2025-12-15T02:59:54.257276Z","shell.execute_reply":"2025-12-15T02:59:54.257299Z"}},"outputs":[],"execution_count":null}]}