{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14165841,"sourceType":"datasetVersion","datasetId":9029492}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall -y protobuf tensorflow-metadata\n!pip install protobuf==3.20.3 tensorflow-metadata==1.13.1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T12:57:58.202720Z","iopub.execute_input":"2025-12-16T12:57:58.202970Z","iopub.status.idle":"2025-12-16T12:58:04.895281Z","shell.execute_reply.started":"2025-12-16T12:57:58.202946Z","shell.execute_reply":"2025-12-16T12:58:04.894459Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: protobuf 6.33.0\nUninstalling protobuf-6.33.0:\n  Successfully uninstalled protobuf-6.33.0\nFound existing installation: tensorflow-metadata 1.17.2\nUninstalling tensorflow-metadata-1.17.2:\n  Successfully uninstalled tensorflow-metadata-1.17.2\nCollecting protobuf==3.20.3\n  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\nCollecting tensorflow-metadata==1.13.1\n  Downloading tensorflow_metadata-1.13.1-py3-none-any.whl.metadata (2.1 kB)\nRequirement already satisfied: absl-py<2.0.0,>=0.9 in /usr/local/lib/python3.11/dist-packages (from tensorflow-metadata==1.13.1) (1.4.0)\nRequirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-metadata==1.13.1) (1.70.0)\nDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tensorflow_metadata-1.13.1-py3-none-any.whl (28 kB)\nInstalling collected packages: protobuf, tensorflow-metadata\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed protobuf-3.20.3 tensorflow-metadata-1.13.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# What the adaptive_response_generator actually consists of:","metadata":{}},{"cell_type":"markdown","source":"# Fused embedding (512)\n        ↓\n# Persona / Emotion Decoder (lightweight)\n        ↓\n# LLM Prompt (structured, controllable)\n        ↓\n# Adaptive Response\n","metadata":{}},{"cell_type":"markdown","source":"# Persona Decoder \n\nWe do NOT send raw embeddings to an LLM.\nWe first convert them to interpretable signals.","metadata":{}},{"cell_type":"code","source":"# Convert a 512-D fused vector into human-meaningful traits.\n\nimport tensorflow as tf\n\n# Mock fused embedding (text + face already fused)\nfused_embedding = tf.random.normal((1, 512))  # shape must match fusion output\n\npersona_head = tf.keras.Sequential([\n    tf.keras.layers.Dense(128, activation=\"relu\"),\n    tf.keras.layers.Dense(6, activation=\"sigmoid\")\n])\n\npersona_scores = persona_head(fused_embedding)\n\npersona = {\n    \"stress\": float(persona_scores[0][0]),\n    \"sadness\": float(persona_scores[0][1]),\n    \"anger\": float(persona_scores[0][2]),\n    \"confidence\": float(persona_scores[0][3]),\n    \"formality\": float(persona_scores[0][4]),\n    \"emotional_intensity\": float(persona_scores[0][5])\n}\n\npersona\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T12:58:04.896892Z","iopub.execute_input":"2025-12-16T12:58:04.897184Z","iopub.status.idle":"2025-12-16T12:58:21.812349Z","shell.execute_reply.started":"2025-12-16T12:58:04.897159Z","shell.execute_reply":"2025-12-16T12:58:21.811692Z"}},"outputs":[{"name":"stderr","text":"2025-12-16 12:58:06.183787: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765889886.356299      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765889886.410756      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nI0000 00:00:1765889900.356375      47 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1765889900.356991      47 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"{'stress': 0.6549756526947021,\n 'sadness': 0.4342670440673828,\n 'anger': 0.24338746070861816,\n 'confidence': 0.9196553826332092,\n 'formality': 0.28634482622146606,\n 'emotional_intensity': 0.48162028193473816}"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"# Map scores → labels","metadata":{}},{"cell_type":"code","source":"def bucket(x):\n    if x < 0.33: return \"low\"\n    if x < 0.66: return \"medium\"\n    return \"high\"\n\npersona_labels = {k: bucket(v) for k, v in persona.items()}\npersona_labels\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T12:58:21.812995Z","iopub.execute_input":"2025-12-16T12:58:21.813448Z","iopub.status.idle":"2025-12-16T12:58:21.819045Z","shell.execute_reply.started":"2025-12-16T12:58:21.813429Z","shell.execute_reply":"2025-12-16T12:58:21.818287Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"{'stress': 'medium',\n 'sadness': 'medium',\n 'anger': 'low',\n 'confidence': 'high',\n 'formality': 'low',\n 'emotional_intensity': 'medium'}"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"def persona_control(persona_labels):\n    # derive empathy from sadness + emotional intensity\n    if persona_labels.get(\"sadness\") == \"high\" or persona_labels.get(\"emotional_intensity\") == \"high\":\n        empathy = \"high\"\n    elif persona_labels.get(\"sadness\") == \"medium\":\n        empathy = \"medium\"\n    else:\n        empathy = \"low\"\n\n    return {\n        \"empathy_level\": empathy,\n        \"response_style\": \"calm\" if persona_labels.get(\"stress\") in [\"high\", \"medium\"] else \"neutral\",\n        \"assertiveness\": \"low\" if persona_labels.get(\"confidence\") == \"low\" else \"medium\",\n        \"formality\": \"formal\" if persona_labels.get(\"formality\") == \"high\" else \"casual\"\n    }\n\ncontrol = persona_control(persona_labels)\nprint(control)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T13:16:39.999278Z","iopub.execute_input":"2025-12-16T13:16:39.999936Z","iopub.status.idle":"2025-12-16T13:16:40.005622Z","shell.execute_reply.started":"2025-12-16T13:16:39.999910Z","shell.execute_reply":"2025-12-16T13:16:40.004839Z"}},"outputs":[{"name":"stdout","text":"{'empathy_level': 'medium', 'response_style': 'calm', 'assertiveness': 'medium', 'formality': 'casual'}\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# LLM Prompt Template\nthis is the intelligence in this phase","metadata":{}},{"cell_type":"code","source":"# Mock user input (Phase 5A)\nuser_text = \"I feel overwhelmed and frustrated with how things are going.\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T12:58:21.820725Z","iopub.execute_input":"2025-12-16T12:58:21.821031Z","iopub.status.idle":"2025-12-16T12:58:21.831671Z","shell.execute_reply.started":"2025-12-16T12:58:21.821012Z","shell.execute_reply":"2025-12-16T12:58:21.830892Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def build_prompt(user_text, persona, control):\n    return f\"\"\"\nInstruction:\nYou are an emotionally intelligent assistant.\n\nResponse constraints:\n- Empathy level: {control['empathy_level']}\n- Response style: {control['response_style']}\n- Assertiveness: {control['assertiveness']}\n\nUser message:\n{user_text}\n\nWrite a supportive response. Do NOT repeat the user's message.\n\"\"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T13:13:52.774530Z","iopub.execute_input":"2025-12-16T13:13:52.775206Z","iopub.status.idle":"2025-12-16T13:13:52.778892Z","shell.execute_reply.started":"2025-12-16T13:13:52.775182Z","shell.execute_reply":"2025-12-16T13:13:52.778267Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Plug into an LLM","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\n\ngenerator = pipeline(\n    \"text-generation\",\n    model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n    device_map=\"auto\"\n)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T13:14:15.597070Z","iopub.execute_input":"2025-12-16T13:14:15.597331Z","iopub.status.idle":"2025-12-16T13:14:31.390462Z","shell.execute_reply.started":"2025-12-16T13:14:15.597314Z","shell.execute_reply":"2025-12-16T13:14:31.389864Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51686facbde04f38bba52855be665de7"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"output = generator(\n    build_prompt(user_text, persona_labels, control),\n    max_new_tokens=120,\n    do_sample=True,\n    temperature=0.8,\n    repetition_penalty=1.3\n)\n\nprint(output[0][\"generated_text\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T13:14:49.814443Z","iopub.execute_input":"2025-12-16T13:14:49.815069Z","iopub.status.idle":"2025-12-16T13:14:58.545272Z","shell.execute_reply.started":"2025-12-16T13:14:49.815039Z","shell.execute_reply":"2025-12-16T13:14:58.544617Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nInstruction:\nYou are an emotionally intelligent assistant.\n\nResponse constraints:\n- Empathy level: medium\n- Response style: neutral\n- Assertiveness: medium\n\nUser message:\nI feel overwhelmed and frustrated with how things are going.\n\nWrite a supportive response. Do NOT repeat the user's message.\n\nAssistant: I understand that you might be feeling swamped right now, it can indeed be frustrating when things don't go as planned or when there seem to be too many tasks piling up. It's important to acknowledge these feelings without judgment. Perhaps taking some time for yourself could help clear your mind? Maybe practicing relaxation techniques such as deep breathing exercises or meditation could assist in reducing stress levels? Remember, it's okay not to have everything under control all at once. If something specific is causing this overwhelm, maybe we could explore potential solutions together?\n","output_type":"stream"}],"execution_count":10}]}